{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageCorruptionMNIST-masknoise05.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VMBoehm/DeNoPa/blob/master/ImageCorruptionMNIST_masknoise05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8gtEmg6uErj",
        "colab_type": "text"
      },
      "source": [
        "**Notebook for reproducing the third example in Section 3.3 (data inputation and denoising)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-Fe5G8m1FTC",
        "colab_type": "code",
        "outputId": "8abb28f1-f724-46b3-f08d-436de2670829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "%pylab inline\n",
        "import pickle"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbM9eAbNvLiP",
        "colab_type": "code",
        "outputId": "36830baf-5c5e-4db7-835c-26c3b486b52a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "! pip install -q https://github.com/dfm/corner.py/archive/master.zip\n",
        "import corner"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     | 10.8MB 2.4MB/s\n",
            "\u001b[?25h  Building wheel for corner (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AEYmOsH1FTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "1f555334-c380-442f-8a74-1659c5d365d4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow_hub as hub\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "from tensorflow.contrib.distributions import softplus_inverse"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0523 08:19:15.748441 140542838114176 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8puPFE90P0aD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_path   = '../modules/decoder1/decoder'\n",
        "encoder_path     = '../modules/encoder1/encoder'\n",
        "nvp_func_path    = '../modules/nvp1/'\n",
        "minima_path      = '../minima/'\n",
        "plot_path        = '../plots/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFzYYSxY1FTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip, zipfile, tarfile\n",
        "import os, shutil, re, string, urllib, fnmatch\n",
        "import pickle as pkl\n",
        "\n",
        "def _download_mnist_realval(dataset):\n",
        "    \"\"\"\n",
        "    Download the MNIST dataset if it is not present.\n",
        "    :return: The train, test and validation set.\n",
        "    \"\"\"\n",
        "    origin = (\n",
        "        'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
        "    )\n",
        "    print('Downloading data from %s' % origin)\n",
        "    urllib.request.urlretrieve(origin, dataset)\n",
        "\n",
        "def _get_datafolder_path():\n",
        "    full_path = os.path.abspath('.')\n",
        "    path = full_path +'/data'\n",
        "    return path\n",
        "\n",
        "def load_mnist_realval(\n",
        "        dataset=_get_datafolder_path()+'/mnist_real/mnist.pkl.gz'):\n",
        "    '''\n",
        "    Loads the real valued MNIST dataset\n",
        "    :param dataset: path to dataset file\n",
        "    :return: None\n",
        "    '''\n",
        "    if not os.path.isfile(dataset):\n",
        "        datasetfolder = os.path.dirname(dataset)\n",
        "        if not os.path.exists(datasetfolder):\n",
        "            os.makedirs(datasetfolder)\n",
        "        _download_mnist_realval(dataset)\n",
        "\n",
        "    f = gzip.open(dataset, 'rb')\n",
        "    train_set, valid_set, test_set = pkl.load(f, encoding='latin1')\n",
        "    f.close()\n",
        "    x_train, targets_train = train_set[0], train_set[1]\n",
        "    x_valid, targets_valid = valid_set[0], valid_set[1]\n",
        "    x_test, targets_test = test_set[0], test_set[1]\n",
        "    return x_train, targets_train, x_valid, targets_valid, x_test, targets_test\n",
        "  \n",
        "x_train, targets_train, x_valid, targets_valid, x_test, targets_test = load_mnist_realval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft_jIh-W1FTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dim    = 28*28\n",
        "data_size   = 1\n",
        "sigma_n     = 0.1\n",
        "hidden_size = 10\n",
        "n_channels  = 1\n",
        "seed        = 777\n",
        "\n",
        "\n",
        "# settings for reconstruction with uncorrupted data\n",
        "# corr_type   = 'none'\n",
        "# num_mnist   = 6\n",
        "# label       = 'uncorrupted'\n",
        "# noise_level = 0.0\n",
        "# num_comp    = 2\n",
        "\n",
        "# settings for reconstrcution with rectangular mask\n",
        "# corr_type   = 'mask'\n",
        "# num_mnist   = 6\n",
        "# label       = 'solidmask'\n",
        "# noise_level = 0.0\n",
        "# num_comp    = 5\n",
        "\n",
        "#settings for reconstruction with sparse mask \n",
        "# corr_type   = 'sparse mask'\n",
        "# num_mnist   = 1\n",
        "# label       = 'sparse95'\n",
        "# noise_level = 0.\n",
        "# num_comp    = 2\n",
        "\n",
        "# settings for reconstruction with noise\n",
        "# corr_type   = 'noise'\n",
        "# num_mnist   = 6\n",
        "# label       = 'noise05'\n",
        "# noise_level = 0.5\n",
        "# num_comp    = 4\n",
        "\n",
        "# settings for reconstruction with noise and mask\n",
        "corr_type   = 'noise+mask'\n",
        "num_mnist   = 6\n",
        "label       = 'masknoise05'\n",
        "noise_level = 0.5\n",
        "num_comp    = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxcZOE0MLGJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_image(image, save=True, directory='./plots/',filename='plotted_image', title='image',vmin=None,vmax=None, mask=None):\n",
        "  \n",
        "  if np.any(mask==None):\n",
        "    mask=np.ones_like(image)\n",
        "  mask = np.reshape(mask,(28,28))\n",
        "  plt.figure()\n",
        "  #plt.title(title)\n",
        "  plt.imshow((image).reshape((28,28))*mask,cmap='gray',vmin=vmin, vmax=vmax)\n",
        "  plt.axis('off')\n",
        "  #plt.colorbar()\n",
        "  if save: \n",
        "    plt.savefig(directory+filename+'.pdf',bbox_inches='tight')\n",
        "  plt.show()\n",
        "  \n",
        "  return True\n",
        "\n",
        "def get_custom_noise(shape, signal_dependent=False, signal =None, sigma_low=0.07, sigma_high=0.22, threshold=0.02 ):\n",
        "  \n",
        "  sigma = np.ones(shape)*sigma_n\n",
        "  \n",
        "  if signal_dependent: \n",
        "    for ii in range(data_size):\n",
        "      sigma[ii][np.where(signal[ii]<=threshold)]= sigma_low\n",
        "      sigma[ii][np.where(signal[ii]>threshold)]= sigma_high\n",
        "      \n",
        "  data_noise = np.ones_like(sigma)*noise_level\n",
        "  \n",
        "  sigma = np.sqrt(sigma**2+data_noise**2)\n",
        "  \n",
        "  return sigma\n",
        "  \n",
        "\n",
        "def make_corrupted_data(x_true, corr_type='mask'):\n",
        "  \n",
        "  mask = np.ones((28,28))\n",
        "  \n",
        "  if corr_type=='mask':\n",
        "    \n",
        "    minx = 10\n",
        "    maxx = 24\n",
        "    \n",
        "    mask[0:28,minx:maxx]=0.\n",
        "    mask = mask.reshape((28*28))\n",
        "    \n",
        "    corr_data = x_true*[mask]\n",
        "    \n",
        "  elif corr_type=='sparse mask':\n",
        "    \n",
        "    mask    = np.ones(data_dim, dtype=int)\n",
        "    percent = 95\n",
        "    np.random.seed(seed+2)\n",
        "    indices = np.random.choice(np.arange(data_dim), replace=False,size=int(percent/100.*data_dim))\n",
        "    print('precentage masked:', len(indices)/data_dim)\n",
        "    mask[indices] =0 \n",
        "  \n",
        "    corr_data = x_true*[mask]\n",
        "    \n",
        "  elif corr_type=='noise':\n",
        "    \n",
        "    np.random.seed(seed+2)\n",
        "    noise = np.random.randn(data_dim*data_size)*noise_level\n",
        "    \n",
        "    corr_data = x_true+noise\n",
        "    \n",
        "  elif corr_type=='noise+mask':\n",
        "    \n",
        "    np.random.seed(seed+2)\n",
        "    noise = np.random.randn(data_dim*data_size)*noise_level\n",
        "    \n",
        "    minx = 14\n",
        "    maxx = 28\n",
        "    \n",
        "    mask[0:28,minx:maxx]=0.\n",
        "    mask = mask.reshape((28*28))\n",
        "    \n",
        "    corr_data = x_true+noise\n",
        "    corr_data = corr_data*[mask]\n",
        "    \n",
        "  elif corr_type=='none':\n",
        "    \n",
        "    corr_data = x_true\n",
        "    \n",
        "  corr_data = np.expand_dims(corr_data,-1)\n",
        "  \n",
        "  mask = mask.flatten()\n",
        "  \n",
        "  return corr_data, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TIQArTJHE87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fwd_pass(generator,nvp,z,mask):\n",
        "  \n",
        "  fwd_z           = nvp({'z_sample':np.zeros((1,hidden_size)),'sample_size':1, 'u_sample':z},as_dict=True)['fwd_pass']\n",
        "\n",
        "  gen_z           = tf.boolean_mask(tf.reshape(generator(fwd_z),[data_size,data_dim,n_channels]),mask, axis=1)\n",
        "\n",
        "  return gen_z\n",
        "\n",
        "\n",
        "def get_likelihood(generator,nvp,z,sigma,mask):\n",
        "  \n",
        "  gen_z           = fwd_pass(generator,nvp,z,mask)\n",
        "  \n",
        "  sigma           = tf.boolean_mask(sigma,mask, axis=1)\n",
        "\n",
        "  likelihood      = tfd.Independent(tfd.MultivariateNormalDiag(loc=gen_z,scale_diag=sigma))\n",
        "\n",
        "  return likelihood\n",
        "\n",
        "def get_prior():\n",
        "  \n",
        "  return tfd.MultivariateNormalDiag(tf.zeros([data_size,hidden_size]), scale_identity_multiplier=1.0, name ='prior')\n",
        "\n",
        "def get_log_posterior(z,x,generator,nvp,sigma,mask, beta):\n",
        "\n",
        "  likelihood      = get_likelihood(generator,nvp,z,sigma,mask)\n",
        "  \n",
        "  prior           = get_prior()\n",
        "  \n",
        "  masked_x        = tf.boolean_mask(x,mask, axis=1)\n",
        "  \n",
        "  log_posterior   = prior.log_prob(z)+likelihood.log_prob(masked_x)*beta\n",
        "  \n",
        "  return log_posterior\n",
        "\n",
        "\n",
        "def get_recon(generator,nvp, z,sigma,mask):\n",
        "  \n",
        "  prob = get_likelihood(generator,nvp, z,sigma,mask)\n",
        "  \n",
        "  recon= prob.mean()\n",
        "  \n",
        "  return recon\n",
        "\n",
        "def get_hessian(func, z):\n",
        "  \n",
        "  hess             = tf.hessians(func,z)\n",
        "  hess             = tf.gather(hess, 0)\n",
        "  \n",
        "  return(tf.reduce_sum(hess, axis = 2 ))\n",
        "\n",
        "\n",
        "def get_GN_hessian(generator,nvp,z,mask,sigma):\n",
        "\n",
        "\n",
        "  gen_z            = fwd_pass(generator,nvp,z,mask)\n",
        "  \n",
        "  sigma            = tf.boolean_mask(sigma,mask, axis=1)\n",
        "\n",
        "  grad_g           = tf.gather(tf.gradients(gen_z/(sigma),z),0)\n",
        "\n",
        "  grad_g2          = tf.einsum('ij,ik->ijk',grad_g,grad_g)\n",
        "\n",
        "  one              = tf.linalg.eye(hidden_size, batch_shape=[data_size],dtype=tf.float32)\n",
        "\n",
        "  hess_GN          = one+grad_g2\n",
        "  \n",
        "  return hess_GN\n",
        "  \n",
        "def compute_covariance(hessian):\n",
        "  \n",
        "  cov = tf.linalg.inv(hessian)\n",
        "  \n",
        "  cov = (cov+tf.linalg.transpose(cov))*0.5\n",
        "  \n",
        "  return cov"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGyi6PVWx1qd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def minimize_posterior(initial_value, x, custom_mask, noise, my_sess, annealing =True):\n",
        "  \n",
        "  ini = np.reshape(initial_value,[data_size,hidden_size])\n",
        "  \n",
        "  my_sess.run(MAP_reset,feed_dict={input_data: x, MAP_ini:ini, mask:custom_mask,sigma_corr:noise})\n",
        "  \n",
        "  pos_def = False\n",
        "\n",
        "  \n",
        "  posterior_loss = []\n",
        "  for lrate, numiter in zip([1e-1,1e-2,1e-3],[10000,5000,3000]):\n",
        "    print('lrate', lrate)\n",
        "    for jj in range(numiter):\n",
        "      if annealing and lrate==1e-1:\n",
        "        inv_T= np.round(0.5*np.exp(-(1.-jj/numiter)),decimals=1)\n",
        "      else:\n",
        "        inv_T= 1.\n",
        "      _, ll = my_sess.run([opt_op_MAP,loss_MAP],feed_dict={input_data: x, mask:custom_mask, sigma_corr:noise, lr: lrate, inverse_T:inv_T})\n",
        "      posterior_loss.append(ll)\n",
        "      if jj%1000==0:\n",
        "        print('iter', jj, 'loss', ll,r'inverse T', inv_T)\n",
        "        \n",
        "  z_value = my_sess.run(MAP,feed_dict={input_data: x, mask:custom_mask, sigma_corr:noise})\n",
        "  \n",
        "  eig     = my_sess.run(tf.linalg.eigvalsh(hessian),feed_dict={input_data: x, mask:custom_mask,sigma_corr:noise})\n",
        "  if np.all(eig>0.):\n",
        "    pos_def = True\n",
        "  \n",
        "  loss    = ll\n",
        "  plt.figure()\n",
        "  plt.plot(posterior_loss)\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('iteration')\n",
        "  plt.show()\n",
        "  \n",
        "  return z_value, loss, pos_def\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiAie-wjUcHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_laplace_sample(num,map_value,x,mymask,noise,my_sess):\n",
        "  \n",
        "  my_sess.run(MAP_reset,feed_dict={MAP_ini:map_value})\n",
        "  my_sess.run(update_mu)\n",
        "  my_sess.run(update_TriL,feed_dict={input_data: x, mask: mymask, sigma_corr:noise})\n",
        "  \n",
        "  samples=[]\n",
        "  for ii in range(num):\n",
        "    my_sess.run(posterior_sample,feed_dict={input_data: x, sigma_corr:noise})\n",
        "    samples.append(my_sess.run(recon,feed_dict={input_data: x, sigma_corr:noise}))\n",
        "    \n",
        "  samples=np.asarray(samples)\n",
        "  return samples\n",
        "\n",
        "def get_gmm_sample(num,x,mymask,noise,my_sess):\n",
        "  \n",
        "  samples=[]\n",
        "  for ii in range(num):\n",
        "    samples.append(my_sess.run(gmm_recon,feed_dict={input_data: x, sigma_corr:noise}))\n",
        "    \n",
        "  samples=np.asarray(samples)\n",
        "  return samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXhLJToHcp7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_samples(samples, mask, title='samples', filename='samples'):\n",
        "  plt.figure(figsize=(5,5))\n",
        "  #plt.title(title)\n",
        "  for i in range(min(len(samples),16)):\n",
        "      subplot(4,4,i+1)\n",
        "      imshow(np.reshape(samples[i,:],(28,28)),vmin=-0.2,vmax=1.2, cmap='gray')\n",
        "      axis('off')\n",
        "  plt.savefig(plot_path+filename+'.pdf',bbox_inches='tight')\n",
        "  plt.show()\n",
        "  \n",
        "  if corr_type in ['mask', 'sparse mask', 'noise+mask']:\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title('masked'+title)\n",
        "    for i in range(min(len(samples),16)):\n",
        "        subplot(4,4,i+1)\n",
        "        imshow(np.reshape(samples[i,0,:,0]*mask,(28,28)),vmin=-0.2,vmax=1.2, cmap='gray')\n",
        "        axis('off')     \n",
        "    plt.savefig(plot_path+filename+'masked.pdf',bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYAt6f7MQSpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_start_values(num, my_sess):\n",
        "  result=[]\n",
        "  for ii in range(num):\n",
        "    result.append(my_sess.run(get_prior().sample()))\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaFIFXBnQ3o-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_chi2(sigma,data,mean,masking=True, mask=None,threshold=0.02):\n",
        "  \n",
        "  if masking:\n",
        "    mask = np.reshape(mask,data.shape)\n",
        "    data = data[np.where(mask==1)]\n",
        "    mean = mean[np.where(mask==1)]\n",
        "    sigma= sigma[np.where(mask==1)]\n",
        "    \n",
        "  \n",
        "  low = min(sigma.flatten())\n",
        "  high= max(sigma.flatten())\n",
        "  \n",
        "  chi2_tot = np.sum((data-mean)**2/sigma**2)\n",
        "  dof_tot  = len(np.squeeze(data))\n",
        "  \n",
        "  if corr_type not in ['noise','noise+mask']:\n",
        "    chi2_low = np.sum((data[np.where(data<=threshold)]-mean[np.where(data<=threshold)])**2/sigma[np.where(data<=threshold)]**2)\n",
        "    dof_low  = len(np.squeeze(data[np.where(data<=threshold)]))\n",
        "    chi2_high= np.sum((data[np.where(data>threshold)]-mean[np.where(data>threshold)])**2/sigma[np.where(data>threshold)]**2)\n",
        "    dof_high = len(np.squeeze(data[np.where(data>threshold)]))\n",
        "  else:\n",
        "    chi2_low = None\n",
        "    dof_low  = None\n",
        "    chi2_high= None\n",
        "    dof_high = None\n",
        "  \n",
        "  return chi2_tot, dof_tot, chi2_low, dof_low, chi2_high, dof_high, masking"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGtEbpIZ2vhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_minima(minima, losses, var):\n",
        "\n",
        "  plt.figure()\n",
        "  plt.title('Minimization result')\n",
        "  plt.plot(np.arange(len(losses)),losses,ls='',marker='o')\n",
        "  plt.xlabel('# iteration')\n",
        "  plt.ylabel('loss')\n",
        "  plt.savefig(plot_path+'minimzation_results_%s.png'%(label),bbox_inches='tight')\n",
        "  plt.show()\n",
        "  \n",
        "  colors = matplotlib.colors.Normalize(vmin=min(losses), vmax=max(losses))\n",
        "  cmap   = matplotlib.cm.get_cmap('Spectral')\n",
        "  \n",
        "  var = np.squeeze(var)\n",
        "  plt.figure()\n",
        "  plt.title('value of hidden variables at minima')\n",
        "  for ii in range(len(minima)):\n",
        "  \n",
        "    yerr_= np.sqrt(var[ii])\n",
        "\n",
        "    plt.errorbar(np.arange(hidden_size),np.squeeze(minima)[ii], marker='o',ls='', c=cmap(colors(losses[ii])), mew=0, yerr=yerr_, label ='%d'%losses[ii])\n",
        "  plt.legend(ncol=4, loc=(1.01,0))\n",
        "  plt.xlabel('# hidden variable')\n",
        "  plt.ylabel('value')\n",
        "  plt.savefig(plot_path+'hidden_values_at_minima_%s.png'%(label),bbox_inches='tight')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80BR72DX58VQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def probe_posterior(minimum, x, noise, mymask, my_sess, filename=label):\n",
        "\n",
        "  _ = my_sess.run(MAP_reset,feed_dict={input_data: x, MAP_ini:minimum, sigma_corr:noise})\n",
        "  _ = my_sess.run(update_mu,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
        "  _ = my_sess.run(update_TriL,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
        "  \n",
        "  exact_hessian = sess.run(hessian,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
        "  approx_hessian= sess.run(GN_hessian,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
        "  ll0 = sess.run(loss_MAP,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
        "  \n",
        "  \n",
        "  plt.figure(figsize=(20,5))\n",
        "  \n",
        "  for nn in np.arange(hidden_size):\n",
        "    H    = exact_hessian[0,nn,nn]\n",
        "    HGN  = approx_hessian[0,nn,nn]\n",
        "\n",
        "    losses=[]\n",
        "    \n",
        "    \n",
        "    subplot(2,5,nn+1)\n",
        "    title('latent space direction %d'%nn)\n",
        "    \n",
        "    Delta   = 0.1\n",
        "    steps   = 1000\n",
        "    delta_z = np.zeros((steps,hidden_size))\n",
        "\n",
        "    delta_z[:,nn] = (np.arange(steps)-steps//2)*Delta/steps\n",
        "    new_ini       = delta_z+minimum\n",
        "\n",
        "    for ii in range(steps):\n",
        "      _ = sess.run(MAP_reset,feed_dict={input_data: x, mask:mymask, MAP_ini:np.expand_dims(new_ini[ii],axis=0), sigma_corr:noise})\n",
        "      ll = sess.run(loss_MAP,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
        "      losses.append(ll)\n",
        "     \n",
        "\n",
        "    \n",
        "\n",
        "    plt.plot(new_ini[:,nn],ll0+H*delta_z[:,nn]**2,label='estimate from exact Hessian')\n",
        "    #plt.plot(new_ini[:,nn],ll0+HGN*delta_z[:,nn]**2,label='estimate from Gauss-Newton')\n",
        "    plt.plot(new_ini[:,nn],losses,label='probed posterior', lw=2)\n",
        "    plt.xlabel('z')\n",
        "    plt.ylabel('negative log posterior')\n",
        "    plt.ylim(min(losses),min(losses)+1.5)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.legend(loc=(1.02,1.7))\n",
        "  \n",
        "  \n",
        "  \n",
        "  plt.savefig(plot_path+'probing_posterior_%s.png'%(filename),bbox_inches='tight')\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrZDSLzEIKrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gmm_parameters(minima, x, noise, mymask, offset):\n",
        "  mu   =[]\n",
        "  w    =[]\n",
        "  sigma=[]\n",
        "  for ii in range(num_comp):\n",
        "\n",
        "    # do Laplace approximation around this minimum\n",
        "    mu+=[minima[ii]]\n",
        "    sess.run(MAP_reset,feed_dict={MAP_ini:minima[ii]})\n",
        "    sigma+=[sess.run(update_TriL,feed_dict={input_data: x, sigma_corr:noise, mask: mymask})]\n",
        "\n",
        "    # correct weighting of different minima according to El20 procedure, with samples at the maxima and well seperated maxima\n",
        "    logdet  = sess.run(tf.linalg.logdet(approx_posterior_laplace.covariance()),feed_dict={input_data: x, sigma_corr:noise, mask: mymask})\n",
        "    logprob = sess.run(nlPost_MAP,feed_dict={input_data: x, sigma_corr:noise, mask: mymask})\n",
        "    w+=[np.exp(0.5*logdet+logprob+offset)]\n",
        "    \n",
        "  print('weights of Gaussian mixtures:', w/np.sum(w))\n",
        "  mu     = np.reshape(np.asarray(mu),[1,num_comp,hidden_size])\n",
        "  sigma  = np.reshape(np.asarray(sigma),[1,num_comp,hidden_size,hidden_size])\n",
        "  w      = np.squeeze(np.asarray(w))\n",
        "                         \n",
        "  return mu, sigma, w\n",
        "                                 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQXNNSN7TecV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_prob_2D_GMM(samples, indices):\n",
        "\n",
        "    samples = samples[:,0,:]\n",
        "\n",
        "    samples = np.hstack((np.expand_dims(samples[:,indices[0]],-1),np.expand_dims(samples[:,indices[1]],-1)))\n",
        "\n",
        "    figure=corner.corner(samples)\n",
        "    axes = np.array(figure.axes).reshape((2, 2))\n",
        "\n",
        "    axes[1,0].set_xlabel('latent space variable %d'%indices[0])\n",
        "    axes[1,0].set_ylabel('latent space variable %d'%indices[1])\n",
        "    plt.savefig(plot_path+'posterior_contour_GMM_%s_latent_space_dir_%d_%d.png'%(label,indices[0],indices[1]),bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yvTEYw44O_5q",
        "outputId": "9720fbd8-ea67-43d4-f055-6e754ec11f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "sigma_corr  = tf.placeholder_with_default(np.ones([data_size,data_dim,n_channels], dtype='float32')*sigma_n,shape=[data_size,data_dim,n_channels])\n",
        "mask        = tf.placeholder_with_default(np.ones([data_dim], dtype='float32'),shape=[data_dim])\n",
        "input_data  = tf.placeholder(shape=[data_size,data_dim,n_channels], dtype=tf.float32)\n",
        "\n",
        "inverse_T   = tf.placeholder_with_default(1., shape=[])\n",
        "lr          = tf.placeholder_with_default(0.001,shape=[])\n",
        "\n",
        "encoder     = hub.Module(encoder_path, trainable=False)\n",
        "generator   = hub.Module(generator_path, trainable=False)\n",
        "nvp_funcs   = hub.Module(nvp_func_path, trainable=False)\n",
        "\n",
        "MAP_ini     = tf.placeholder_with_default(tf.zeros([data_size,hidden_size]),shape=[data_size,hidden_size])\n",
        "MAP         = tf.Variable(MAP_ini)\n",
        "MAP_reset   = tf.stop_gradient(MAP.assign(MAP_ini))\n",
        "\n",
        "nlPost_MAP  = get_log_posterior(MAP, input_data, generator,nvp_funcs, sigma_corr,mask, inverse_T)\n",
        "loss_MAP    = -tf.reduce_mean(nlPost_MAP)\n",
        "\n",
        "optimizer   = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "\n",
        "opt_op_MAP  = optimizer.minimize(loss_MAP, var_list=[MAP])\n",
        "\n",
        "recon_MAP   = get_recon(generator,nvp_funcs, MAP,sigma_corr,mask)\n",
        "\n",
        "hessian     = get_hessian(-nlPost_MAP,MAP)\n",
        "\n",
        "GN_hessian  = get_GN_hessian(generator,nvp_funcs,MAP,mask,sigma_corr)\n",
        "\n",
        "ini_val  = np.ones((data_size,(hidden_size *(hidden_size +1)) // 2),dtype=np.float32)\n",
        "with tf.variable_scope(\"Laplace_Posterior\",reuse=tf.AUTO_REUSE):\n",
        "  mu_new      = tf.Variable(np.ones((data_size,hidden_size),dtype=np.float32), dtype=np.float32)\n",
        "  sigma_new_t = ini_val\n",
        "  sigma_new_t2= tf.Variable(tfd.matrix_diag_transform(tfd.fill_triangular(sigma_new_t), transform=tf.nn.softplus),dtype=tf.float32)\n",
        "  \n",
        "approx_posterior_laplace = tfd.MultivariateNormalTriL(loc=mu_new,scale_tril=sigma_new_t2)\n",
        "\n",
        "update_mu          = mu_new.assign(MAP)\n",
        "covariance         = compute_covariance(hessian)\n",
        "variance           = tf.linalg.diag_part(covariance)[0]\n",
        "update_TriL        = sigma_new_t2.assign(tf.linalg.cholesky(covariance))\n",
        "\n",
        "posterior_sample   = approx_posterior_laplace.sample()\n",
        "\n",
        "recon              = get_recon(generator,nvp_funcs, posterior_sample ,sigma_corr,mask)\n",
        "\n",
        "ini_val2    = np.ones((data_size,num_comp,(hidden_size *(hidden_size +1)) // 2),dtype=np.float32)\n",
        "with tf.variable_scope(\"corrupted/gmm\",reuse=tf.AUTO_REUSE):\n",
        "  mu_gmm      = tf.Variable(np.ones((data_size,num_comp,hidden_size)), dtype=np.float32)\n",
        "  sigma_gmm   = tf.Variable(tfd.fill_triangular(ini_val2))\n",
        "  w_gmm       = tf.Variable(np.ones((num_comp))/num_comp, dtype=np.float32)\n",
        "  \n",
        "sigma_gmmt    = tfd.matrix_diag_transform(sigma_gmm, transform=tf.nn.softplus)\n",
        "w_positive    = tf.math.softplus(w_gmm)\n",
        "w_rescaled    = tf.squeeze(w_positive/tf.reduce_sum(w_positive))\n",
        "\n",
        "gmm           = tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(probs=w_rescaled),components_distribution=tfd.MultivariateNormalTriL(loc=mu_gmm,scale_tril=sigma_gmmt))\n",
        "\n",
        "mu_ini        = tf.placeholder_with_default(tf.zeros([data_size,num_comp,hidden_size]),shape=[data_size,num_comp,hidden_size])\n",
        "sigma_ini     = tf.placeholder_with_default(tf.ones([data_size,num_comp,hidden_size, hidden_size]),shape=[data_size,num_comp,hidden_size, hidden_size])\n",
        "w_ini         = tf.placeholder_with_default(tf.ones([num_comp])/num_comp,shape=[num_comp])\n",
        "\n",
        "update_w      = tf.stop_gradient(w_gmm.assign(softplus_inverse(w_ini)))\n",
        "update_mugmm  = tf.stop_gradient(mu_gmm.assign(mu_ini))\n",
        "update_TriLgmm= tf.stop_gradient(sigma_gmm.assign(tfd.matrix_diag_transform(sigma_ini, transform=softplus_inverse)))\n",
        "\n",
        "gmm_sample    = gmm.sample()\n",
        "gmm_recon     = get_recon(generator,nvp_funcs, gmm_sample ,sigma_corr,mask)\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0523 08:34:44.663484 140542838114176 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0523 08:34:44.919940 140542838114176 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0523 08:34:46.774694 140542838114176 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0523 08:34:47.066202 140542838114176 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0523 08:34:51.178848 140542838114176 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0523 08:34:51.500307 140542838114176 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0523 08:34:52.842120 140542838114176 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0523 08:34:53.195766 140542838114176 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0523 08:34:54.049401 140542838114176 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0523 08:34:55.010332 140542838114176 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Soh1tnGH1FTW",
        "colab_type": "code",
        "outputId": "40d91f46-1684-42ef-fed1-44c18716ca62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1362
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  \n",
        "  truth = x_test[num_mnist:num_mnist+data_size]\n",
        "  plot_image(truth, directory=plot_path, filename='truth_%s'%label, title='truth')\n",
        "  \n",
        "  data, custom_mask = make_corrupted_data(truth, corr_type=corr_type)\n",
        "  plot_image(data, directory=plot_path, filename='input_data_%s'%label, title='data')\n",
        "  plot_image(custom_mask, directory=plot_path, filename='mask_data_%s'%label, title='mask')\n",
        "  \n",
        "  noise = get_custom_noise(data.shape, signal_dependent=False, signal=truth)\n",
        "  plot_image(noise, directory=plot_path, filename='noise_%s'%label, title='noise')\n",
        "  \n",
        "  \n",
        "  tf.random.set_random_seed(seed)\n",
        "  inits = get_random_start_values(10, sess)\n",
        "  \n",
        "  try:\n",
        "    minima, min_loss, min_var, recons = pickle.load(open(minima_path+'minima_%s.pkl'%label,'rb'))\n",
        "  except:\n",
        "  \n",
        "    minima  =[]\n",
        "    min_loss=[]\n",
        "    min_var =[]\n",
        "    recons  =[]\n",
        "    for jj,init in enumerate(inits):\n",
        "      print('progress in %', jj/len(inits)*100)\n",
        "      min_z, min_l, pos_def    = minimize_posterior(init, data,custom_mask,noise,sess)\n",
        "      rec                      = sess.run(recon_MAP, feed_dict={sigma_corr:noise})\n",
        "      var                      = sess.run(variance, feed_dict={input_data: data,mask:custom_mask,sigma_corr:noise})\n",
        "  \n",
        "      plot_image(rec, directory=plot_path, filename='recon_%s_minimum%d'%(label,jj), title='reconstruction with loss %.1f'%min_l)\n",
        "      print(min_z)\n",
        "      if pos_def:\n",
        "        print('hessian postive definite')\n",
        "        minima.append(min_z)\n",
        "        min_loss.append(min_l)\n",
        "        min_var.append(var)\n",
        "        recons.append(rec)\n",
        "    \n",
        "    order    = np.argsort(min_loss)\n",
        "    min_loss = np.asarray(min_loss)[order]\n",
        "    minima   = np.asarray(minima)[order]\n",
        "    min_var  = np.asarray(min_var)[order]\n",
        "    \n",
        "    pickle.dump([minima, min_loss, min_var,recons],open(minima_path+'minima_%s.pkl'%label,'wb'))\n",
        "\n",
        "  plot_minima(minima, min_loss, min_var)\n",
        "\n",
        "  \n",
        "  chi2s = get_chi2(noise,data,recons[0],masking=True, mask=custom_mask)\n",
        "  \n",
        "  \n",
        "  print('total chi2 of lowest minimum', '%.1f'%chi2s[0], 'on', '%.1f'%chi2s[1] ,'pixels')\n",
        "  try:\n",
        "    print('chi2 of lowest minimum, low pixel amplitude regions', '%.1f'%chi2s[2], 'on', '%.1f'%chi2s[3], 'pixels')\n",
        "    print('chi2 of lowest minimum, high pixel amplitude regions', '%.1f'%chi2s[4], 'on', '%.1f'%chi2s[5], 'pixels')\n",
        "  except:\n",
        "    pass\n",
        "  \n",
        "  lowest_minimum = sess.run(MAP_reset, feed_dict={MAP_ini:minima[0]})\n",
        "  rec     = sess.run(recon_MAP, feed_dict={sigma_corr:noise})\n",
        "  plot_image(rec, directory=plot_path, filename='lowest_minimum_%s'%(label), title='reconstruction', vmin=0, vmax=1)\n",
        "  if corr_type in ['mask', 'sparse mask', 'noise+mask']:\n",
        "    plot_image(rec, directory=plot_path, filename='lowest_minimum_%s_masked'%(label), title='masked reconstruction', vmin=0, vmax=1, mask = custom_mask)\n",
        "  samples = get_laplace_sample(16,minima[0],data,custom_mask,noise,sess)\n",
        "  plot_samples(samples, custom_mask, title='Samples from Laplace approximation', filename='samples_laplace_deepest_minimum_%s'%label)\n",
        "  \n",
        "  \n",
        "  probe_posterior(minima[0], data, noise, custom_mask, sess)\n",
        "  \n",
        "  mu_, sigma_, w_ = get_gmm_parameters([minima[0],minima[11]], data, noise, custom_mask, min_loss[0])\n",
        "  _ = sess.run([update_w, update_mugmm,update_TriLgmm], feed_dict={mu_ini:mu_, w_ini:w_, sigma_ini:sigma_ })\n",
        "  \n",
        "  samples = get_gmm_sample(16,data,custom_mask,noise,sess)\n",
        "  plot_samples(samples, custom_mask, title='GMM samples', filename='gmm_samples_%s'%label)\n",
        "  \n",
        "  more_samples = []\n",
        "  for ii in range(10000):\n",
        "    more_samples+=[sess.run(gmm_sample,feed_dict={input_data: data, sigma_corr:noise})]\n",
        "  more_samples=np.asarray(more_samples)\n",
        "  \n",
        "  for indices in [[0,1],[1,2],[3,8]]:\n",
        "    plot_prob_2D_GMM(more_samples, indices)\n",
        "  "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEP8Ae6Qji3rRingAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTDB2wARjB0wwdsAEYwdMMHbABGMHTPwP3SWUsZlgvscAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAAwBJREFUeJzt2MEJAkAMAEEj9t9ybMCPnzt0Zxq4\nQFgCN7v7AP7f8/YAwBlihwixQ4TYIULsEPE6/J6v/x8zM7dH4Eu7+3FpLjtEiB0ixA4RYocIsUOE\n2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtE\niB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFD\nhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7\nRIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwix\nQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQ\nO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocI\nsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaI\nEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKH\nCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2\niBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFi\nhwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEzO7engE4wGWHCLFDhNghQuwQ\nIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIt7v\n5gvrUfe1IgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAAv1JREFUeJzt2LENwzAMAMEwyP4r0wu4tQzk70o2\nYvMgoNndD/D/vm8vAJwhdogQO0SIHSLEDhG/k4/NjK9/eNjuzt3cZYcIsUOE2CFC7BAhdogQO0SI\nHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE\n2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtE\niB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFD\nhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7\nRIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwix\nQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQ\nO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocI\nsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaI\nEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKH\nCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2\niBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYoeI2d23dwAOcNkhQuwQIXaIEDtEiB0i\nxA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHiAv1Agvr\neLZQmwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-c2a0734601f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0minits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_start_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-1331055bcf1a>\u001b[0m in \u001b[0;36mget_random_start_values\u001b[0;34m(num, my_sess)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUUVYB2Lex8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ye6CFt1Ku0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}